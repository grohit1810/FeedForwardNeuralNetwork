{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 01\n",
    "\n",
    "Submitted By: <br>\n",
    "Name: G Rohit <br>\n",
    "Student Id: 19233292 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries and defining a method to load any existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, random, csv\n",
    "random.seed(1026847926404610461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_file):\n",
    "    \"\"\"\n",
    "    Load a dumped neural network model\n",
    "    \n",
    "    Parameters: \n",
    "    model_file: filename of the dumped network object\n",
    "    Return: the object of the network.\n",
    "    \"\"\"\n",
    "    return pickle.load(open(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Implementation of Fully Connected Feed Forward Neural Network\n",
    "\n",
    "The given code is an implementation of Fully Connected Feed Forward Neural Network with Back Propagation with the following given characteristics: \n",
    "- The model is implemented using the reference of class learning material for Deep Learning\n",
    "- Has only one hidden layer and it accepts variable number of input node, hidden layer node and output nodes\n",
    "- the user can provide the values for learning rate(0.01 by default) and the activation function(relu by default, can accept relu, sigmoid and tanh as activation function) to the model\n",
    "- Uses HE Initialization to initialize the weights of the nodes\n",
    "- Uses softmax as the activation function for the output layer as it returns the probability of each output class \n",
    "- The model is optimised using Stochastic Gradient Descent for convergence of loss function\n",
    "- Model is saved after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedFeedForwardNN(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, activation = 'relu'):\n",
    "        \"\"\"\n",
    "        Constructor for the network. This function initializes the weight matrices using HE initialization\n",
    "        and sets up necessary variables to be used while tarining and prediction\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: the number of input neurons\n",
    "        hidden_size: the number of hidden neurons\n",
    "        output_size: the number of output neurons i.e. number of classes in the data\n",
    "        learning_rate: the learning_rate used while training the weights. Default value = 0.01\n",
    "        activation: the activation function to be used in the hidden layer of the network. Default: 'relu'\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/(self.input_size)) # Weight for input layer to hidden layer\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/(self.hidden_size)) # Weight for hidden layer to output layer\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden layer bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output layer bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = {'relu': self._relu, 'sigmoid': self._sigmoid, 'tanh': self._tanh}\n",
    "        self.derivative_activations = {'relu': self._relu_derivative, 'sigmoid': self._sigmoid_derivative, 'tanh': self._tanh_derivative}\n",
    "        try:\n",
    "            self.activation_fn = self.activations[activation]\n",
    "            self.activation_fn_derivative = self.derivative_activations[activation]\n",
    "        except:\n",
    "            print(\"Supports : 'sigmoid', 'tanh', 'relu' activations only. Please use only one of these.\")\n",
    "    \n",
    "    #activation functions \n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def _sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    #derivative of activation functions\n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        This function performs forward propagation \n",
    "        Parameter:\n",
    "        X: input\n",
    "        \n",
    "        Returns: hidden layer activations values, final softmax probs \n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h\n",
    "        hidden_activations = self.activation_fn(Z) # perform non-linearity on the input data\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s) #softmax values\n",
    "        return hidden_activations, prob_values\n",
    "    \n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        This function implements backward propagation i.e. calculate error in weights in each layer.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input\n",
    "        target_class: output target class\n",
    "        hidden_activations: hidden activation from forward pass\n",
    "        prob_values: softmax probabilities of output from forward pass\n",
    "        \n",
    "        Returns: error in weights matrices and error in biases\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        #error calculation for the weight matrix of hidden layer to output layer and bias of output layer\n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        #error calculation for the weight matrix of input layer to hidden layer and bias of hidden layer\n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)\n",
    "        delta_h_error = self.activation_fn_derivative(hidden_activations) * delta_h \n",
    "        delta_bias_h += delta_h_error\n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        \n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "    \n",
    "    def _update_weights(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the network\n",
    "        \n",
    "        Parameters:\n",
    "        dWxh: error for weight matrix from input layer to hidden layer\n",
    "        dbh: error for bias for hidden layer\n",
    "        dWhy: error for weight matrix from hidden layer to output layer\n",
    "        dby: error for bias for output layer\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.W_xh -= self.learning_rate * delta_W_xh\n",
    "        self.bias_h -= self.learning_rate * delta_bias_h\n",
    "        self.W_hy -= self.learning_rate * delta_W_hy\n",
    "        self.bias_y -= self.learning_rate * delta_bias_y\n",
    "        \n",
    "    def _calc_mean_loss(self, total_loss, num_samples):\n",
    "        \"\"\"\n",
    "        Calculate the mean loss for the current epoch\n",
    "        \n",
    "        Parameters:\n",
    "        total_loss: sum of all loss calculated for one epoch\n",
    "        num_samples: total number of training sample\n",
    "        \n",
    "        Returns: mean loss for an epoch\n",
    "        \"\"\"\n",
    "        return 1./num_samples * total_loss\n",
    "\n",
    "    def train(self, inp, targets, num_epochs,model_file = \"NNModel.pkl\"):\n",
    "        \"\"\"\n",
    "        This function trains the network i.e. by doing a forward pass then a backward prop and then subsequently \n",
    "        update the weights with the errors calculated in the backward pass\n",
    "        \n",
    "        Parameters:\n",
    "        inp: list of input samples\n",
    "        targets: list of corresponding training output classes \n",
    "        num_epochs: number of epochs for training the network\n",
    "        model_file: filename of the pickle to save the model after training\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        for epoch_no in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inp_sample_no in range(len(inp)): #looping through each input sample\n",
    "                # forward propagation\n",
    "                hidden_activations, prob_values = self._forward_propagation(inp[inp_sample_no])\n",
    "                total_loss += -np.log(prob_values[targets[inp_sample_no], 0])\n",
    "\n",
    "                # backward propagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inp[inp_sample_no], targets[inp_sample_no], hidden_activations, prob_values)\n",
    "\n",
    "                # update the weights of the model with the error calculated in back prop\n",
    "                self._update_weights(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "                \n",
    "            print(\"Epoch \", epoch_no, \": Loss: \", self._calc_mean_loss(total_loss, len(inp) ))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function predicts the output class i.e. performs forward propagations and returns the class \n",
    "        with maximum probabilty from the softmax(output layer) layer probs. \n",
    "        \n",
    "        Parameters:\n",
    "        X: input to test\n",
    "        \n",
    "        Return: the predicted output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_filename):\n",
    "        \"\"\"\n",
    "        This function dumps the model to a file. So that it can loaded later.\n",
    "        \n",
    "        Parameters:\n",
    "        model_filename: filename of the pickled model\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Testing the Implementation on circles dataset\n",
    "\n",
    "The model implemented above is tested on the circles dataset in the following steps: \n",
    "- The input file is read and parsed line by line to create an array containing the input values in form (x1, x2) and the output class labels\n",
    "- The model implemented above is run with 2 input and 4 hidden layers\n",
    "- The values of learning rate and activation function used is 0.01 and relu respectively\n",
    "- The data is split in 90:10 ratio as training and testing sets and the model is trained for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"circles500.csv\")\n",
    "index = 0\n",
    "circles_data = []\n",
    "for line in file :\n",
    "    if index == 0:\n",
    "        index +=1\n",
    "        continue\n",
    "    x0,x1,output_class = line.split(',')\n",
    "    current_row = {}\n",
    "    inp = np.asarray([float(x0),float(x1)])\n",
    "    current_row[\"inp\"] = inp\n",
    "    current_row[\"out\"] = int(output_class)\n",
    "    circles_data.append(current_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test Neural network on circles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : Loss:  0.6060232463689772\n",
      "Epoch  1 : Loss:  0.5480278683347299\n",
      "Epoch  2 : Loss:  0.4972651738582458\n",
      "Epoch  3 : Loss:  0.44399671793666684\n",
      "Epoch  4 : Loss:  0.3915368449019963\n",
      "Epoch  5 : Loss:  0.345542884166262\n",
      "Epoch  6 : Loss:  0.3065142274357497\n",
      "Epoch  7 : Loss:  0.27397224700802353\n",
      "Epoch  8 : Loss:  0.24662720399658802\n",
      "Epoch  9 : Loss:  0.22405241001766987\n",
      "Epoch  10 : Loss:  0.20541895035739655\n",
      "Epoch  11 : Loss:  0.18971459508026461\n",
      "Epoch  12 : Loss:  0.17629199866485634\n",
      "Epoch  13 : Loss:  0.1648290627192087\n",
      "Epoch  14 : Loss:  0.15481255480031708\n",
      "Epoch  15 : Loss:  0.14596609099527788\n",
      "Epoch  16 : Loss:  0.13821752393988523\n",
      "Epoch  17 : Loss:  0.1312775958929311\n",
      "Epoch  18 : Loss:  0.1251074803601537\n",
      "Epoch  19 : Loss:  0.11952280357515518\n",
      "Epoch  20 : Loss:  0.11441875514113833\n",
      "Epoch  21 : Loss:  0.10984076427771625\n",
      "Epoch  22 : Loss:  0.10560211721126085\n",
      "Epoch  23 : Loss:  0.10174991798656308\n",
      "Epoch  24 : Loss:  0.09817287067108932\n",
      "Epoch  25 : Loss:  0.09486164916578625\n",
      "Epoch  26 : Loss:  0.09175542208325876\n",
      "Epoch  27 : Loss:  0.08892005085250401\n",
      "Epoch  28 : Loss:  0.08625213352063739\n",
      "Epoch  29 : Loss:  0.08375609329343517\n",
      "Epoch  30 : Loss:  0.08141301015467309\n",
      "Epoch  31 : Loss:  0.07921428928261498\n",
      "Epoch  32 : Loss:  0.07708320315683377\n",
      "Epoch  33 : Loss:  0.07517154000484079\n",
      "Epoch  34 : Loss:  0.07332319677228114\n",
      "Epoch  35 : Loss:  0.07156476027534878\n",
      "Epoch  36 : Loss:  0.06988985643692443\n",
      "Epoch  37 : Loss:  0.068274476952962\n",
      "Epoch  38 : Loss:  0.06674810538248326\n",
      "Epoch  39 : Loss:  0.06529626307806129\n",
      "Epoch  40 : Loss:  0.06386351182013988\n",
      "Epoch  41 : Loss:  0.06254037663351147\n",
      "Epoch  42 : Loss:  0.06132628212797956\n",
      "Epoch  43 : Loss:  0.06006199029993316\n",
      "Epoch  44 : Loss:  0.0588651787941753\n",
      "Epoch  45 : Loss:  0.05769187130940655\n",
      "Epoch  46 : Loss:  0.056544614123628996\n",
      "Epoch  47 : Loss:  0.05545183289888138\n",
      "Epoch  48 : Loss:  0.05441151978740798\n",
      "Epoch  49 : Loss:  0.05341871217017449\n",
      "Accuracy :  100.0\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(2,4,2)\n",
    "inputs=[]\n",
    "targets=[]\n",
    "num_epocs, counter = 50, 0\n",
    "training_size = int(0.9*len(circles_data))\n",
    "for i in range(0,len(circles_data)):\n",
    "    targets.append(circles_data[i]['out'])\n",
    "    inputs.append(circles_data[i]['inp'])\n",
    "nn.train(inputs[:training_size], targets[:training_size], num_epocs, model_file=\"Circles_NN_Model.pkl\")\n",
    "counter=0\n",
    "for j in range(len(circles_data[training_size:])):\n",
    "    s=nn.predict(circles_data[j+training_size]['inp'])\n",
    "    s1=circles_data[j+training_size]['out']\n",
    "    if s == s1:\n",
    "        counter+=1\n",
    "print(\"Accuracy : \",((counter*1.0)/(j+1))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Testing the implementation on CIFAR-10 dataset\n",
    "\n",
    "The implementation is now tested on CIFAR-10 data set to distinguish **cats and deers** using the following steps:\n",
    "- The data has ~1000 images for a class in each batch, we will use all the 5 training batches to train the model(1000x5x2), i.e., 10000 images and will test on the test set(1000x2) of 2000 images\n",
    "- All the deer and cat samples from the input and the test batches are read, converted to grayscale and normalised for image classification\n",
    "- Each image is 32x32, hence there are 1024 input nodes in the neural network, 8 nodes in the hidden layer\n",
    "- Sigmoid function is used as the activation function for the hidden layer\n",
    "- Number of epochs used is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"cifar-10-batches-py\"\n",
    "train_files = [folder+\"/data_batch_\"+str(i) for i in range(1,6) ]\n",
    "test_file = folder+\"/test_batch\"\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "def Convert_rgb_to_grayscale_and_normalize(image_vector):\n",
    "    grayscale_vector = []\n",
    "    individual_spec_length = int(len(image_vector)/3) \n",
    "    for i in range(individual_spec_length):\n",
    "        red_value = image_vector[i]\n",
    "        green_value = image_vector[i + individual_spec_length]\n",
    "        blue_value = image_vector[i+ (2*individual_spec_length)]\n",
    "        # New grayscale image = ( (0.3 * R) + (0.59 * G) + (0.11 * B) ).\n",
    "        grayscale_value = ((0.3*red_value) + (0.59*green_value) + (0.11*blue_value))\n",
    "        grayscale_vector.append(grayscale_value)\n",
    "    return NormalizeData(np.asarray(grayscale_vector))\n",
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "deer_samples, cat_samples = {}, {}\n",
    "deer_samples['train'], cat_samples['train'] = [], []\n",
    "cat_samples['test'], deer_samples['test'] = [], []\n",
    "for file in train_files :\n",
    "    train_data = unpickle(file)\n",
    "    for i in range(len(train_data[b'labels'])):\n",
    "        if train_data[b'labels'][i] == 3:\n",
    "            cat_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "        if train_data[b'labels'][i] == 4:\n",
    "            deer_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "\n",
    "test_data = unpickle(test_file)\n",
    "for i in range(0,len(test_data[b'labels'])):\n",
    "    if test_data[b'labels'][i] == 3:\n",
    "        cat_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "    if test_data[b'labels'][i] == 4:\n",
    "        deer_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "pickle.dump(cat_samples, open(\"cat_samples.pkl\", \"wb\"))\n",
    "pickle.dump(deer_samples, open(\"deer_samples.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test CIFAR dataset with neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : Loss:  0.6653246366807261\n",
      "Epoch  1 : Loss:  0.6385096703634113\n",
      "Epoch  2 : Loss:  0.6148030954550864\n",
      "Epoch  3 : Loss:  0.5982069157624755\n",
      "Epoch  4 : Loss:  0.5910848693283969\n",
      "Epoch  5 : Loss:  0.5862203408074587\n",
      "Epoch  6 : Loss:  0.5810175738748422\n",
      "Epoch  7 : Loss:  0.5766272481261265\n",
      "Epoch  8 : Loss:  0.5732648201258922\n",
      "Epoch  9 : Loss:  0.5691692625226028\n",
      "Epoch  10 : Loss:  0.5647321186504732\n",
      "Epoch  11 : Loss:  0.562268680200155\n",
      "Epoch  12 : Loss:  0.5597859383598162\n",
      "Epoch  13 : Loss:  0.5573255726403048\n",
      "Epoch  14 : Loss:  0.5553178355402721\n",
      "Epoch  15 : Loss:  0.5535307179908535\n",
      "Epoch  16 : Loss:  0.5505375014556831\n",
      "Epoch  17 : Loss:  0.5487360964572647\n",
      "Epoch  18 : Loss:  0.5469050562266669\n",
      "Epoch  19 : Loss:  0.5444993439414461\n",
      "Epoch  20 : Loss:  0.5425490541740479\n",
      "Epoch  21 : Loss:  0.5411646665952998\n",
      "Epoch  22 : Loss:  0.5391945893512248\n",
      "Epoch  23 : Loss:  0.5379617289838875\n",
      "Epoch  24 : Loss:  0.5367014717226369\n",
      "Epoch  25 : Loss:  0.5353440257566191\n",
      "Epoch  26 : Loss:  0.5338050692615186\n",
      "Epoch  27 : Loss:  0.5323856433938163\n",
      "Epoch  28 : Loss:  0.5297795917184595\n",
      "Epoch  29 : Loss:  0.5284697843710364\n",
      "Epoch  30 : Loss:  0.5273586948124533\n",
      "Epoch  31 : Loss:  0.5256838400760228\n",
      "Epoch  32 : Loss:  0.5249872934606817\n",
      "Epoch  33 : Loss:  0.5238916541804572\n",
      "Epoch  34 : Loss:  0.5223209716177202\n",
      "Epoch  35 : Loss:  0.5210907685628708\n",
      "Epoch  36 : Loss:  0.519990613001057\n",
      "Epoch  37 : Loss:  0.5190256381425795\n",
      "Epoch  38 : Loss:  0.5170608410229105\n",
      "Epoch  39 : Loss:  0.515989849479082\n",
      "Epoch  40 : Loss:  0.5146061214487683\n",
      "Epoch  41 : Loss:  0.513005420216575\n",
      "Epoch  42 : Loss:  0.513443936559909\n",
      "Epoch  43 : Loss:  0.5122758458835334\n",
      "Epoch  44 : Loss:  0.5117380740397136\n",
      "Epoch  45 : Loss:  0.509541012677369\n",
      "Epoch  46 : Loss:  0.509001837618782\n",
      "Epoch  47 : Loss:  0.5098685374790094\n",
      "Epoch  48 : Loss:  0.5082721560036371\n",
      "Epoch  49 : Loss:  0.5069490795242498\n",
      "Correctly identified test cases :  1326\n",
      "Accuracy :  66.3\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(1024,8,2, activation = 'relu')\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "nn.train(inputs, targets, num_epocs)\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Enhancement Implementation - L2 regularizing\n",
    "\n",
    "To address the problem of overfitting and increase the model's performance, regularizing weights using L2 Regularization is implemented, as more training data cannot be provided to the model. In L2 regularization high weight values are penalised, reducing the activation values of the non-linear layer and hereby reducing the complexity of the network. <br> \n",
    "Regularization of weights is applied on 20% of the training data.<br>\n",
    "\n",
    "For testing the CIFAR dataset the final configuration used is: (1024,16,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNNWithRegulization(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network with L2 Regularization as enhancement. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, regularization_lambda=0.01, activation = 'relu'):\n",
    "        \"\"\"\n",
    "        Constructor for the network. This function initializes the weight matrices using HE initialization\n",
    "        and sets up necessary variables to be used while tarining and prediction\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: the number of input neurons\n",
    "        hidden_size: the number of hidden neurons\n",
    "        output_size: the number of output neurons i.e. number of classes in the data\n",
    "        learning_rate: the learning_rate used while training the weights. Default: 0.01\n",
    "        regulaziation_lambda: the regularization lambda used for regulizing the weights. Default: 0.01\n",
    "        activation: the activation function to be used in the hidden layer of the network. Default: 'relu'\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/(self.input_size)) # Weight for input layer to hidden layer\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/(self.hidden_size)) # Weight for hidden layer to output layer\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden layer bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output layer bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "        self.activations = {'relu': self._relu, 'sigmoid': self._sigmoid, 'tanh': self._tanh}\n",
    "        self.derivative_activations = {'relu': self._relu_derivative, 'sigmoid': self._sigmoid_derivative, 'tanh': self._tanh_derivative}\n",
    "        try:\n",
    "            self.activation_fn = self.activations[activation]\n",
    "            self.activation_fn_derivative = self.derivative_activations[activation]\n",
    "        except:\n",
    "            print(\"Supports : 'sigmoid', 'tanh', 'relu' activations only. Please use only one of these.\")\n",
    "    #activation functions\n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def _sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    #derivative of activation functions\n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        This function performs forward propagation \n",
    "        Parameter:\n",
    "        X: input\n",
    "        \n",
    "        Returns: hidden layer activations values, final softmax probs \n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h\n",
    "        hidden_activations = self.activation_fn(Z) # perform non-linearity on the input data\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s) # softmax values\n",
    "        return hidden_activations, prob_values\n",
    "    \n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        This function implements backward propagation i.e. calculate error in weights in each layer.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input\n",
    "        target_class: output target class\n",
    "        hidden_activations: hidden activation from forward pass\n",
    "        prob_values: softmax probabilities of output from forward pass\n",
    "        \n",
    "        Returns: error in weights matrices and error in biases\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        #error calculation for the weight matrix of hidden layer to output layer and bias of output layer\n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        #error calculation for the weight matrix of input layer to hidden layer and bias of hidden layer\n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)\n",
    "        delta_h_error = self.activation_fn_derivative(hidden_activations) * delta_h \n",
    "        delta_bias_h += delta_h_error\n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        \n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "\n",
    "    def _regularize_weights(self, delta_W_hy, delta_W_xh):\n",
    "        \"\"\"\n",
    "        Add regularization terms to the weights\n",
    "        \n",
    "        Parameters:\n",
    "        delta_W_hy: error for weight matrix from hidden layer to output layer\n",
    "        delta_W_xh: error for weight matrix from input layer to hidden layer\n",
    "        Returns: updated errors with regularization factor\n",
    "        \"\"\"\n",
    "        delta_W_hy += self.regularization_lambda * self.W_hy\n",
    "        delta_W_xh += self.regularization_lambda * self.W_xh\n",
    "        return delta_W_hy, delta_W_xh\n",
    "\n",
    "    def _update_weights(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the network\n",
    "        \n",
    "        Parameters:\n",
    "        delta_W_xh: error for weight matrix from input layer to hidden layer\n",
    "        delta_bias_h: error for bias for hidden layer\n",
    "        delta_W_hy: error for weight matrix from hidden layer to output layer\n",
    "        delta_bias_y: error for bias for output layer\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.W_xh -= self.learning_rate * delta_W_xh\n",
    "        self.bias_h -= self.learning_rate * delta_bias_h\n",
    "        self.W_hy -= self.learning_rate * delta_W_hy\n",
    "        self.bias_y -= self.learning_rate * delta_bias_y\n",
    "\n",
    "    def _calc_loss_with_regularization(self, total_loss, num_samples, regularization_used=None):\n",
    "        \"\"\"\n",
    "        Calculate the loss for the current epoch with regularization quotient\n",
    "        \n",
    "        Parameters:\n",
    "        total_loss: sum of all loss calculated for one epoch\n",
    "        num_samples: total number of training samples\n",
    "        regularization_used: the type of regularization used. Supports only L2 till now. Defualt: None\n",
    "        \n",
    "        Returns: mean loss for an epoch with regularization quotient\n",
    "        \"\"\"\n",
    "        if regularization_used == 'L2':\n",
    "            total_loss += (self.regularization_lambda/2) * (np.sum(np.square(self.W_xh)) + np.sum(np.square(self.W_hy)))\n",
    "            return 1./num_samples * total_loss\n",
    "        else:\n",
    "            return 1./num_samples * total_loss\n",
    "\n",
    "    def train(self, inp, targets, regularization_data, num_epochs,model_file = \"NNModelWithL2.pkl\", regularization=None):\n",
    "        \"\"\"\n",
    "        This function trains the network i.e. by doing a forward pass then a backward prop and then subsequently \n",
    "        update the weights with the errors calculated in the backward pass\n",
    "        \n",
    "        Parameters:\n",
    "        inp: list of input samples\n",
    "        targets: list of corresponding training output classes \n",
    "        num_epochs: number of epochs for training the network\n",
    "        model_file: filename of the pickle to save the model after training\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        for epoch_no in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for inp_sample_no in range(len(inp)):\n",
    "                # forward propagation\n",
    "                hidden_activations, prob_values = self._forward_propagation(inp[inp_sample_no])\n",
    "                total_loss += -np.log(prob_values[targets[inp_sample_no], 0])\n",
    "\n",
    "                # backward propagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inp[inp_sample_no], targets[inp_sample_no], hidden_activations, prob_values)\n",
    "                    \n",
    "                # update the weights of the model with the error calculated in back prop\n",
    "                self._update_weights(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            #Regularization samples\n",
    "            for i in range(len(regularization_data[0])):\n",
    "                # forward propagation\n",
    "                hidden_activations, prob_values = self._forward_propagation(regularization_data[0][i])\n",
    "                total_loss += -np.log(prob_values[regularization_data[1][i], 0])\n",
    "\n",
    "                # backward propagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(regularization_data[0][i], regularization_data[1][i], hidden_activations, prob_values)\n",
    "\n",
    "                if regularization == 'L2':\n",
    "                    delta_W_hy, delta_W_xh = self._regularize_weights(delta_W_hy, delta_W_xh)\n",
    "\n",
    "                # update the weights of the model with the error calculated in back prop\n",
    "                self._update_weights(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            print(\"Epoch \", epoch_no, \": Loss: \", self._calc_loss_with_regularization(total_loss, len(inp)+len(regularization_data[0]), regularization))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function predicts the output class i.e. performs forward propagations and returns the class \n",
    "        with maximum probabilty from the softmax(output layer) layer probs. \n",
    "        \n",
    "        Parameters:\n",
    "        X: input to test\n",
    "        \n",
    "        Return: the predicted output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        This function dumps the model to a file. So that it can loaded later.\n",
    "        \n",
    "        Parameters:\n",
    "        model_filename: filename of the pickled model\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : Loss:  0.6653260932885826\n",
      "Epoch  1 : Loss:  0.616173938489198\n",
      "Epoch  2 : Loss:  0.599900779422258\n",
      "Epoch  3 : Loss:  0.592413738673131\n",
      "Epoch  4 : Loss:  0.5873619432071301\n",
      "Epoch  5 : Loss:  0.5834187165307422\n",
      "Epoch  6 : Loss:  0.5801078331261527\n",
      "Epoch  7 : Loss:  0.5772708359207818\n",
      "Epoch  8 : Loss:  0.5747560435685782\n",
      "Epoch  9 : Loss:  0.5724214231239936\n",
      "Epoch  10 : Loss:  0.5701917389369742\n",
      "Epoch  11 : Loss:  0.567965948638451\n",
      "Epoch  12 : Loss:  0.5657507682036469\n",
      "Epoch  13 : Loss:  0.5635731353988305\n",
      "Epoch  14 : Loss:  0.5614816446887412\n",
      "Epoch  15 : Loss:  0.5595765611471618\n",
      "Epoch  16 : Loss:  0.5578649923221459\n",
      "Epoch  17 : Loss:  0.5563368694620691\n",
      "Epoch  18 : Loss:  0.5549771568888267\n",
      "Epoch  19 : Loss:  0.5537731392853983\n",
      "Epoch  20 : Loss:  0.5527159811140003\n",
      "Epoch  21 : Loss:  0.5517884083212761\n",
      "Epoch  22 : Loss:  0.5509613498311312\n",
      "Epoch  23 : Loss:  0.5501946230352354\n",
      "Epoch  24 : Loss:  0.5494437431448452\n",
      "Epoch  25 : Loss:  0.5487057450314732\n",
      "Epoch  26 : Loss:  0.5479784686982755\n",
      "Epoch  27 : Loss:  0.5472218189162154\n",
      "Epoch  28 : Loss:  0.5463705913588797\n",
      "Epoch  29 : Loss:  0.5452690721461787\n",
      "Epoch  30 : Loss:  0.5442647765000576\n",
      "Epoch  31 : Loss:  0.5435467977965538\n",
      "Epoch  32 : Loss:  0.5429473399803046\n",
      "Epoch  33 : Loss:  0.5424421208648501\n",
      "Epoch  34 : Loss:  0.542015380104455\n",
      "Epoch  35 : Loss:  0.5416519170676676\n",
      "Epoch  36 : Loss:  0.5413386449543283\n",
      "Epoch  37 : Loss:  0.5410651899049079\n",
      "Epoch  38 : Loss:  0.5408230742876576\n",
      "Epoch  39 : Loss:  0.5406046236451068\n",
      "Epoch  40 : Loss:  0.540402156357543\n",
      "Epoch  41 : Loss:  0.5402079660289911\n",
      "Epoch  42 : Loss:  0.5400145801601751\n",
      "Epoch  43 : Loss:  0.5398150468596569\n",
      "Epoch  44 : Loss:  0.5396046549872205\n",
      "Epoch  45 : Loss:  0.5393820947883592\n",
      "Epoch  46 : Loss:  0.5391488155421034\n",
      "Epoch  47 : Loss:  0.5389076103199572\n",
      "Epoch  48 : Loss:  0.5386632066256895\n",
      "Epoch  49 : Loss:  0.5384220973847702\n",
      "Correctly identified test cases :  1348\n",
      "Accuracy :  67.4\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedNNWithRegulization(1024,16,2, activation ='sigmoid')\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "training_size = int(0.8*len(targets))\n",
    "regularization_size = int(0.2*len(targets))\n",
    "nn.train(inputs[:training_size], targets[:training_size],(inputs[training_size:],targets[training_size:]) ,num_epocs, regularization='L2')\n",
    "# nn.train(inputs, targets, num_epocs, regularization='L2')\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Enchancement Implementation - Adding another hidden layer\n",
    "\n",
    "As network configuration was (1024,8,2) with 1024 input, 8 hidden and 2 output nodes, a good image embedding is not acheived in a single hidden layer. Adding another hidden layer might improve the emebdding of the input image, thus making the model deep. Implementation below uses two hidden layers to train and classify the images and attempts to solve the problem of underfitting.\n",
    "\n",
    "For testing the CIFAR dataset the final configuration used is: (1024,32,8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNNWithTwoHiddenLayer(object):\n",
    "    \"\"\"\n",
    "    Implementation of Fully Connected Neural Network with two hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, learning_rate=0.01, activation1 = 'relu', activation2 ='tanh'):\n",
    "        \"\"\"\n",
    "        Constructor for the network. This function initializes the weight matrices using HE initialization\n",
    "        and sets up necessary variables to be used while tarining and prediction\n",
    "        \n",
    "        Parameters:\n",
    "        input_size: the number of input neurons\n",
    "        hidden1_size: the number of neurons in hidden layer 1\n",
    "        hidden2_size: the number of neurons in hidden layer 1\n",
    "        output_size: the number of output neurons i.e. number of classes in the data\n",
    "        learning_rate: the learning_rate used while training the weights. Default value = 0.01\n",
    "        activation1: the activation function to be used in the first hidden layer of the network. Default: 'relu'\n",
    "        activation2: the activation function to be used in the second hidden layer of the network. Default: 'tanh'\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size \n",
    "        self.output_size = output_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.Wxh1 = np.random.randn(self.hidden1_size, self.input_size) * np.sqrt(2/(self.hidden1_size+self.input_size)) # Weight for input layer to first hidden layer\n",
    "        self.Wh1h2 = np.random.randn(self.hidden2_size,self.hidden1_size) * np.sqrt(2/(self.hidden2_size+self.hidden1_size)) # Weight for first hidden layer to second hidden layer\n",
    "        self.Wh2y = np.random.randn(self.output_size, self.hidden2_size) * np.sqrt(2/(self.output_size+self.hidden2_size)) # Weight for second hidden layer to output layer\n",
    "        self.bh1 = np.zeros((self.hidden1_size, 1)) # first hidden layer bias\n",
    "        self.bh2 = np.zeros((self.hidden2_size, 1)) # second hidden layer bias\n",
    "        self.by = np.zeros((self.output_size, 1)) # output bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = {'relu': self._relu, 'sigmoid': self._sigmoid, 'tanh': self._tanh}\n",
    "        self.derivative_activations = {'relu': self._relu_derivative, 'sigmoid': self._sigmoid_derivative, 'tanh': self._tanh_derivative}\n",
    "        self.activation1 = self.activations[activation1]\n",
    "        self.activation1_derivative = self.derivative_activations[activation1]\n",
    "        self.activation2 = self.activations[activation2]\n",
    "        self.activation2_derivative = self.derivative_activations[activation2]\n",
    "    #activation functions\n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def _sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    #derivative of activation functions\n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        This function performs forward propagation \n",
    "        Parameter:\n",
    "        X: input\n",
    "        \n",
    "        Returns: hidden layer activations values, final softmax probs \n",
    "        \"\"\"\n",
    "        Z = np.dot(self.Wxh1, np.reshape(X,(len(X),1))) + self.bh1\n",
    "        h1_a = self.activation1(Z)# perform non-linearity on the input data\n",
    "        Z = np.dot(self.Wh1h2, h1_a) + self.bh2\n",
    "        h2_a = self.activation2(Z)# perform non-linearity on the first hidden layer activations\n",
    "        y_a = np.exp(np.dot(self.Wh2y, h2_a) + self.by)\n",
    "        probs = y_a/np.sum(y_a)\n",
    "        return h1_a, h2_a, probs\n",
    "\n",
    "    def _back_propagation(self, X, t, h1_a, h2_a, probs):\n",
    "        \"\"\"\n",
    "        This function implements backward propagation i.e. calculate error in weights in each layer.\n",
    "        \n",
    "        Parameters:\n",
    "        X: input\n",
    "        t: output target class\n",
    "        hidden_activations: hidden activation from forward pass\n",
    "        prob_values: softmax probabilities of output from forward pass\n",
    "        \n",
    "        Returns: error in weights matrices and error in biases\n",
    "        \"\"\"\n",
    "        dWxh1, dWh1h2, dWh2y = np.zeros_like(self.Wxh1), np.zeros_like(self.Wh1h2), np.zeros_like(self.Wh2y)\n",
    "        dbh1, dbh2, dby = np.zeros_like(self.bh1), np.zeros_like(self.bh2), np.zeros_like(self.by)\n",
    "\n",
    "        #error calculation in the last layer\n",
    "        dy = np.copy(probs)\n",
    "        dy[t] -= 1\n",
    "        dWh2y = np.dot(dy, h2_a.T)\n",
    "        dby += dy\n",
    "\n",
    "        #error calculation in second hidden layer\n",
    "        dh2 = np.dot(self.Wh2y.T,dy)\n",
    "        dh2raw = self.activation2_derivative(h2_a) * dh2 \n",
    "        dbh2 += dh2raw\n",
    "        dWh1h2 += np.dot(dh2raw, h1_a.T)\n",
    "\n",
    "        #error calculation in the first hidden layer\n",
    "        dh1 = np.dot(self.Wh1h2.T, dh2raw) \n",
    "        dh1raw = self.activation1_derivative(h1_a) * dh1\n",
    "        dbh1 += dh1raw\n",
    "        dWxh1 += np.dot(dh1raw, np.reshape(X, (len(X), 1)).T)\n",
    "        \n",
    "        return dWxh1, dWh1h2, dWh2y, dbh1, dbh2, dby\n",
    "\n",
    "    def _update_weights(self, dWxh1, dbh1, dWh1h2, dbh2, dWh2y, dby):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the network\n",
    "        \n",
    "        Parameters:\n",
    "        dWxh1: error for weight matrix from input layer to first hidden layer\n",
    "        dbh1: error for bias for first hidden layer\n",
    "        dWh1h2: error for weight matrix from first hidden layer to second hidden layer\n",
    "        dbh2: error for bias for second hidden layer\n",
    "        dWh2y: error for weight matrix from second hidden layer to output layer\n",
    "        dby: error for bias for output layer\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        self.Wxh1 -= self.learning_rate * dWxh1\n",
    "        self.bh1 -= self.learning_rate * dbh1\n",
    "        self.Wh1h2 -= self.learning_rate * dWh1h2\n",
    "        self.bh2 -= self.learning_rate * dbh2\n",
    "        self.Wh2y -= self.learning_rate * dWh2y\n",
    "        self.by -= self.learning_rate * dby\n",
    "\n",
    "    def _calc_mean_loss(self, total_loss, num_samples):\n",
    "        \"\"\"\n",
    "        Calculate the mean loss for the current epoch\n",
    "        \n",
    "        Parameters:\n",
    "        total_loss: sum of all loss calculated for one epoch\n",
    "        num_examples: total number of training sample\n",
    "        \n",
    "        Returns: mean loss for an epoch\n",
    "        \"\"\"\n",
    "        return 1./num_samples * total_loss\n",
    "\n",
    "    def train(self, inp, targets, num_epochs, model_file = 'TwoLayerNNModel.pkl'):\n",
    "        \"\"\"\n",
    "        This function trains the network i.e. by doing a forward pass then a backward prop and then subsequently \n",
    "        update the weights with the errors calculated in the backward pass\n",
    "        \n",
    "        Parameters:\n",
    "        inp: list of input samples\n",
    "        targets: list of corresponding training output classes \n",
    "        num_epochs: number of epochs for training the network\n",
    "        model_file: filename of the pickle to save the model after training\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        for epoch_no in range(num_epochs):#looping through each input sample\n",
    "            total_loss = 0\n",
    "            for inp_sample_no in range(len(inp)):\n",
    "                # forward propagation\n",
    "                h1_a, h2_a, probs = self._forward_propagation(inp[inp_sample_no])\n",
    "                total_loss += -np.log(probs[targets[inp_sample_no], 0])\n",
    "\n",
    "                # backward propagation\n",
    "                dWxh1, dWh1h2, dWh2y, dbh1, dbh2, dby = self._back_propagation(inp[inp_sample_no], targets[inp_sample_no], h1_a, h2_a, probs)\n",
    "\n",
    "                # update the weights of the model with the error calculated in back prop\n",
    "                self._update_weights(dWxh1, dbh1, dWh1h2, dbh2, dWh2y, dby)\n",
    "\n",
    "            print(\"Epoch \", epoch_no, \", Loss: \", self._calc_mean_loss(total_loss, len(inp)))\n",
    "        self.save(model_file)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function predicts the output class i.e. performs forward propagations and returns the class \n",
    "        with maximum probabilty from the softmax(output layer) layer probs. \n",
    "        \n",
    "        Parameters:\n",
    "        X: input to test\n",
    "        \n",
    "        Return: the predicted output class\n",
    "        \"\"\"\n",
    "        h1_a, h2_a, probs = self._forward_propagation(X)\n",
    "        return np.argmax(probs)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        This function dumps the model to a file. So that it can loaded later.\n",
    "        \n",
    "        Parameters:\n",
    "        model_filename: filename of the pickled model\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 , Loss:  0.6555052825090637\n",
      "Epoch  1 , Loss:  0.6048372690987214\n",
      "Epoch  2 , Loss:  0.584576707112227\n",
      "Epoch  3 , Loss:  0.5721347269857121\n",
      "Epoch  4 , Loss:  0.5605916505223151\n",
      "Epoch  5 , Loss:  0.5495321042672581\n",
      "Epoch  6 , Loss:  0.5396838327807107\n",
      "Epoch  7 , Loss:  0.5335342620270234\n",
      "Epoch  8 , Loss:  0.5271578664233311\n",
      "Epoch  9 , Loss:  0.5169966591286511\n",
      "Epoch  10 , Loss:  0.5086317445540611\n",
      "Epoch  11 , Loss:  0.502199243724382\n",
      "Epoch  12 , Loss:  0.4955777629039728\n",
      "Epoch  13 , Loss:  0.4887897001959324\n",
      "Epoch  14 , Loss:  0.4819299627115247\n",
      "Epoch  15 , Loss:  0.4763177836668254\n",
      "Epoch  16 , Loss:  0.47254835741931184\n",
      "Epoch  17 , Loss:  0.4658534117000471\n",
      "Epoch  18 , Loss:  0.46147561262595804\n",
      "Epoch  19 , Loss:  0.4543343540454525\n",
      "Epoch  20 , Loss:  0.44640331767450625\n",
      "Epoch  21 , Loss:  0.44768524116970265\n",
      "Epoch  22 , Loss:  0.4420000131393422\n",
      "Epoch  23 , Loss:  0.43293050895537427\n",
      "Epoch  24 , Loss:  0.43592675798612684\n",
      "Epoch  25 , Loss:  0.43064763994066146\n",
      "Epoch  26 , Loss:  0.42128038958316066\n",
      "Epoch  27 , Loss:  0.4202568395268748\n",
      "Epoch  28 , Loss:  0.4072440057290567\n",
      "Epoch  29 , Loss:  0.40546373120387946\n",
      "Epoch  30 , Loss:  0.40585548313518727\n",
      "Epoch  31 , Loss:  0.3992035723025781\n",
      "Epoch  32 , Loss:  0.392575948482433\n",
      "Epoch  33 , Loss:  0.38837080698475956\n",
      "Epoch  34 , Loss:  0.3828159497168114\n",
      "Epoch  35 , Loss:  0.3811147099328045\n",
      "Epoch  36 , Loss:  0.3781695416357859\n",
      "Epoch  37 , Loss:  0.3713766587636063\n",
      "Epoch  38 , Loss:  0.3784741531095483\n",
      "Epoch  39 , Loss:  0.36876431093917217\n",
      "Epoch  40 , Loss:  0.3637264243206077\n",
      "Epoch  41 , Loss:  0.3635149228495892\n",
      "Epoch  42 , Loss:  0.3575487078356833\n",
      "Epoch  43 , Loss:  0.3548931307153373\n",
      "Epoch  44 , Loss:  0.3499585322099727\n",
      "Epoch  45 , Loss:  0.3480039328200649\n",
      "Epoch  46 , Loss:  0.3537618352920885\n",
      "Epoch  47 , Loss:  0.3437690938364926\n",
      "Epoch  48 , Loss:  0.3416316593403495\n",
      "Epoch  49 , Loss:  0.3292436216220925\n",
      "Correctly identified test cases :  1439\n",
      "Accuracy :  71.95\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedNNWithTwoHiddenLayer(1024,32,8,2,activation1='relu',activation2='sigmoid')\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "nn.train(inputs, targets, num_epocs)\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Neural network from scratch: https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6\n",
    "- HE Initialisation: https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "- Converting image to grayscale: https://www.tutorialspoint.com/dip/grayscale_to_rgb_conversion.htm\n",
    "- L2 Regularisation: https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "- Improve Neural Network: https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3\n",
    "- Andrew Ng Notes: https://medium.com/@keonyonglee/bread-and-butter-from-deep-learning-by-andrew-ng-course-1-neural-networks-and-deep-learning-41563b8fc5d8\n",
    "- Lecture Notes for Deep Learning by Prof Michael Madden, NUIG\n",
    "- Lecture Notes for Machine Learning by Prof Anantharaman Narayana Iyer, PESIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
