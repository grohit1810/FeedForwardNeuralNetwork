{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, random, csv\n",
    "random.seed(1026847926404610461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_file):\n",
    "    \"\"\"\n",
    "    Loads the network from the model_file\n",
    "    :param model_file: file onto which the network is saved\n",
    "    :return: the network\n",
    "    \"\"\"\n",
    "    return pickle.load(open(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of fully connected feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedFeedForwardNN(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, activation = 'relu'):\n",
    "        \"\"\"\n",
    "        Initialize the network with input, output sizes, weights and biases\n",
    "        :param input_dim: input dim\n",
    "        :param hidden_size: number of hidden units\n",
    "        :param output_dim: output dim\n",
    "        :param learning_rate: learning rate alpha\n",
    "        :param reg_lambda: regularization rate lambda\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/self.input_size) # Weight matrix for input to hidden\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/self.hidden_size) # Weight matrix for hidden to output\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output bias\n",
    "        self.learning_rate = learning_rate\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = self._sigmoid\n",
    "            self.activation_derivative = self._sigmoid_derivative\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = self._tanh\n",
    "            self.activation_derivative = self._tanh_derivative\n",
    "        else:\n",
    "            self.activation = self._relu\n",
    "            self.activation_derivative = self._relu_derivative\n",
    "    \n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the ANN\n",
    "        :param X: input\n",
    "        :return: hidden activations, softmax probabilities for the output\n",
    "        \"\"\"\n",
    "        #hidden_activations = np.tanh(np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h)\n",
    "        Z = np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h\n",
    "        hidden_activations = self.activation(Z)\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s)\n",
    "        return hidden_activations, prob_values\n",
    "\n",
    "    def _update_parameter(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases during gradient descent\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param dbh: bias derivative from input to hidden\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dby: bias derivative from hidden to output\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.W_xh += -self.learning_rate * delta_W_xh\n",
    "        self.bias_h += -self.learning_rate * delta_bias_h\n",
    "        self.W_hy += -self.learning_rate * delta_W_hy\n",
    "        self.bias_y += -self.learning_rate * delta_bias_y\n",
    "\n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        Implementation of the backpropagation algorithm\n",
    "        :param X: input\n",
    "        :param t: target\n",
    "        :param h_a: hidden activation from forward pass\n",
    "        :param probs: softmax probabilities of output from forward pass\n",
    "        :return: dWxh, dWhy, dbh, dby\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)  # backprop into h\n",
    "        delta_h_error = self.activation_derivative(hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        #delta_h_error = (1 - hidden_activations * hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        delta_bias_h += delta_h_error\n",
    "        \n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "\n",
    "    def _calc_smooth_loss(self, loss, len_examples):\n",
    "        \"\"\"\n",
    "        Calculate the smoothened loss over the set of examples\n",
    "        :param loss: loss calculated for a sample\n",
    "        :param len_examples: total number of samples in training + validation set\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: smooth loss\n",
    "        \"\"\"\n",
    "        return 1./len_examples * loss\n",
    "\n",
    "    def train(self, inputs, targets, num_epochs,model_file = \"NNModel.pkl\"):\n",
    "        \"\"\"\n",
    "        Trains the network by performing forward pass followed by backpropagation\n",
    "        :param inputs: list of training inputs\n",
    "        :param targets: list of corresponding training targets\n",
    "        :param validation_data: tuple of (X,y) where X and y are inputs and targets\n",
    "        :param num_epochs: number of epochs for training the model\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for k in range(num_epochs):\n",
    "            loss = 0\n",
    "            for i in range(len(inputs)):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(inputs[i])\n",
    "                loss += -np.log(prob_values[targets[i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inputs[i], targets[i], hidden_activations, prob_values)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "\n",
    "            if k%1 == 0:\n",
    "                print(\"Epoch \" + str(k) + \" : Loss = \" + str(self._calc_smooth_loss(loss, len(inputs))))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an input X, emi\n",
    "        :param X: test input\n",
    "        :return: the output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        # return probs\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        Saves the network to a file\n",
    "        :param model_file: name of the file where the network should be saved\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a test cell, to test the working of NN. Need to remove this in the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.5354056555242742\n",
      "Epoch 1 : Loss = 0.060108421946899755\n",
      "Epoch 2 : Loss = 0.022738730816128705\n",
      "Epoch 3 : Loss = 0.01335841806943601\n",
      "Epoch 4 : Loss = 0.009277469210338958\n",
      "Epoch 5 : Loss = 0.007016084900997115\n",
      "Epoch 6 : Loss = 0.005594879611682645\n",
      "Epoch 7 : Loss = 0.004626158258098615\n",
      "Epoch 8 : Loss = 0.003927156955039262\n",
      "Epoch 9 : Loss = 0.0034010281391607853\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nn = FullyConnectedFeedForwardNN(4,8,4)\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(1000):\n",
    "    num = random.randint(0,3)\n",
    "    inp = np.zeros((4,))\n",
    "    inp[num] = 1\n",
    "    inputs.append(inp)\n",
    "    targets.append(num)\n",
    "\n",
    "nn.train(inputs, targets, 10)\n",
    "print(nn.predict([1,0,0,0]))\n",
    "print(nn.predict([0,1,0,0]))\n",
    "print(nn.predict([0,0,1,0]))\n",
    "print(nn.predict([0,0,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 : Load circles dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"circles500.csv\")\n",
    "index = 0\n",
    "circles_data = []\n",
    "for line in file :\n",
    "    if index == 0:\n",
    "        index +=1\n",
    "        continue\n",
    "    x0,x1,output_class = line.split(',')\n",
    "    current_row = {}\n",
    "    inp = np.asarray([float(x0),float(x1)])\n",
    "    current_row[\"inp\"] = inp\n",
    "    current_row[\"out\"] = int(output_class)\n",
    "    circles_data.append(current_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Neural network model on circles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.7025880111488243\n",
      "Epoch 1 : Loss = 0.608607716845455\n",
      "Epoch 2 : Loss = 0.5439252338026608\n",
      "Epoch 3 : Loss = 0.48485546709701666\n",
      "Epoch 4 : Loss = 0.4290302797227898\n",
      "Epoch 5 : Loss = 0.37832266108753876\n",
      "Epoch 6 : Loss = 0.3336297175578382\n",
      "Epoch 7 : Loss = 0.29581315347690107\n",
      "Epoch 8 : Loss = 0.26465771315449843\n",
      "Epoch 9 : Loss = 0.23925130909523612\n",
      "Epoch 10 : Loss = 0.2185215333214398\n",
      "Epoch 11 : Loss = 0.20133624843299514\n",
      "Epoch 12 : Loss = 0.1869104812062117\n",
      "Epoch 13 : Loss = 0.17461174439755342\n",
      "Epoch 14 : Loss = 0.16405793488691012\n",
      "Epoch 15 : Loss = 0.15487270930612346\n",
      "Epoch 16 : Loss = 0.14684731001162368\n",
      "Epoch 17 : Loss = 0.13973439589768544\n",
      "Epoch 18 : Loss = 0.1334004920507077\n",
      "Epoch 19 : Loss = 0.1276925126569793\n",
      "Epoch 20 : Loss = 0.12253107544320974\n",
      "Epoch 21 : Loss = 0.1178447481925892\n",
      "Epoch 22 : Loss = 0.11356079881164771\n",
      "Epoch 23 : Loss = 0.10963959796227489\n",
      "Epoch 24 : Loss = 0.10602096624169757\n",
      "Epoch 25 : Loss = 0.10262745036971278\n",
      "Epoch 26 : Loss = 0.09945561635410811\n",
      "Epoch 27 : Loss = 0.09649622132447636\n",
      "Epoch 28 : Loss = 0.093687842912511\n",
      "Epoch 29 : Loss = 0.09101652759392957\n",
      "Epoch 30 : Loss = 0.08853698291014712\n",
      "Epoch 31 : Loss = 0.0862758676606869\n",
      "Epoch 32 : Loss = 0.08405393496208982\n",
      "Epoch 33 : Loss = 0.08194522244166084\n",
      "Epoch 34 : Loss = 0.07996358124529651\n",
      "Epoch 35 : Loss = 0.0780606651136206\n",
      "Epoch 36 : Loss = 0.07626176471776198\n",
      "Epoch 37 : Loss = 0.0744512034166527\n",
      "Epoch 38 : Loss = 0.07270105825674944\n",
      "Epoch 39 : Loss = 0.0710398689175694\n",
      "Epoch 40 : Loss = 0.06942030946287125\n",
      "Epoch 41 : Loss = 0.06784702340969126\n",
      "Epoch 42 : Loss = 0.06633254495370747\n",
      "Epoch 43 : Loss = 0.06487318226144384\n",
      "Epoch 44 : Loss = 0.06348066821582787\n",
      "Epoch 45 : Loss = 0.06216221761355185\n",
      "Epoch 46 : Loss = 0.06089866301906156\n",
      "Epoch 47 : Loss = 0.05965301546490202\n",
      "Epoch 48 : Loss = 0.05847902213463083\n",
      "Epoch 49 : Loss = 0.05737029863124377\n",
      "Accuracy :  98.0\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(2,4,2)\n",
    "inputs=[]\n",
    "targets=[]\n",
    "num_epocs, counter = 50, 0\n",
    "training_size = int(0.9*len(circles_data))\n",
    "for i in range(0,len(circles_data)):\n",
    "    targets.append(circles_data[i]['out'])\n",
    "    inputs.append(circles_data[i]['inp'])\n",
    "nn.train(inputs[:training_size], targets[:training_size], num_epocs, model_file=\"Circles_NN_Model.pkl\")\n",
    "counter=0\n",
    "for j in range(len(circles_data[training_size:])):\n",
    "    s=nn.predict(circles_data[j+training_size]['inp'])\n",
    "    s1=circles_data[j+training_size]['out']\n",
    "    if s == s1:\n",
    "        counter+=1\n",
    "print(\"Accuracy : \",((counter*1.0)/(j+1))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 : Load CIFAR Dataset. \n",
    "Will load cat and deer train(all batches) and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"CIFAR_Dataset\"\n",
    "train_files = [folder+\"/data_batch_\"+str(i) for i in range(1,6) ]\n",
    "test_file = folder+\"/test_batch\"\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "def Convert_rgb_to_grayscale_and_normalize(image_vector):\n",
    "    grayscale_vector = []\n",
    "    individual_spec_length = int(len(image_vector)/3) \n",
    "    for i in range(individual_spec_length):\n",
    "        red_value = image_vector[i]\n",
    "        green_value = image_vector[i + individual_spec_length]\n",
    "        blue_value = image_vector[i+ (2*individual_spec_length)]\n",
    "        # New grayscale image = ( (0.3 * R) + (0.59 * G) + (0.11 * B) ).\n",
    "        grayscale_value = ((0.3*red_value) + (0.59*green_value) + (0.11*blue_value))\n",
    "        grayscale_vector.append(grayscale_value)\n",
    "    return NormalizeData(np.asarray(grayscale_vector))\n",
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "deer_samples, cat_samples = {}, {}\n",
    "deer_samples['train'], cat_samples['train'] = [], []\n",
    "cat_samples['test'], deer_samples['test'] = [], []\n",
    "for file in train_files :\n",
    "    train_data = unpickle(file)\n",
    "    for i in range(len(train_data[b'labels'])):\n",
    "        if train_data[b'labels'][i] == 3:\n",
    "            cat_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "        if train_data[b'labels'][i] == 4:\n",
    "            deer_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "\n",
    "test_data = unpickle(test_file)\n",
    "for i in range(0,len(test_data[b'labels'])):\n",
    "    if test_data[b'labels'][i] == 3:\n",
    "        cat_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "    if test_data[b'labels'][i] == 4:\n",
    "        deer_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "pickle.dump(cat_samples, open(\"cat_samples.pkl\", \"wb\"))\n",
    "pickle.dump(deer_samples, open(\"deer_samples.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CIFAR dataset with neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.6481057288194713\n",
      "Epoch 1 : Loss = 0.6018717754663375\n",
      "Epoch 2 : Loss = 0.5900084945414481\n",
      "Epoch 3 : Loss = 0.5827116520897151\n",
      "Epoch 4 : Loss = 0.5766489593936781\n",
      "Epoch 5 : Loss = 0.5714321069858236\n",
      "Epoch 6 : Loss = 0.5663203621348406\n",
      "Epoch 7 : Loss = 0.5616899022594215\n",
      "Epoch 8 : Loss = 0.5553569051801397\n",
      "Epoch 9 : Loss = 0.5505750779053323\n",
      "Epoch 10 : Loss = 0.5466899040892228\n",
      "Epoch 11 : Loss = 0.5431942098618681\n",
      "Epoch 12 : Loss = 0.5378948125562636\n",
      "Epoch 13 : Loss = 0.5326189396530271\n",
      "Epoch 14 : Loss = 0.5303578385247281\n",
      "Epoch 15 : Loss = 0.5257168378796676\n",
      "Epoch 16 : Loss = 0.5224526811882015\n",
      "Epoch 17 : Loss = 0.5170895226103492\n",
      "Epoch 18 : Loss = 0.5134179212020604\n",
      "Epoch 19 : Loss = 0.5093199379948439\n",
      "Epoch 20 : Loss = 0.5044218509333056\n",
      "Epoch 21 : Loss = 0.5006521026013915\n",
      "Epoch 22 : Loss = 0.498055499285154\n",
      "Epoch 23 : Loss = 0.49354810416510886\n",
      "Epoch 24 : Loss = 0.48918365091739263\n",
      "Epoch 25 : Loss = 0.48376252412561693\n",
      "Epoch 26 : Loss = 0.48064675852218924\n",
      "Epoch 27 : Loss = 0.47590968910599263\n",
      "Epoch 28 : Loss = 0.4750569126359854\n",
      "Epoch 29 : Loss = 0.47038985434129327\n",
      "Epoch 30 : Loss = 0.4670809497212653\n",
      "Epoch 31 : Loss = 0.464216800818507\n",
      "Epoch 32 : Loss = 0.4575942167929543\n",
      "Epoch 33 : Loss = 0.4558622050096776\n",
      "Epoch 34 : Loss = 0.45166791067495055\n",
      "Epoch 35 : Loss = 0.45211060272150116\n",
      "Epoch 36 : Loss = 0.451060874767491\n",
      "Epoch 37 : Loss = 0.44413079821898355\n",
      "Epoch 38 : Loss = 0.43959438774698095\n",
      "Epoch 39 : Loss = 0.435454067111098\n",
      "Epoch 40 : Loss = 0.43475192625314296\n",
      "Epoch 41 : Loss = 0.43352746397231734\n",
      "Epoch 42 : Loss = 0.4283789433776224\n",
      "Epoch 43 : Loss = 0.4257874589777393\n",
      "Epoch 44 : Loss = 0.42567830548182806\n",
      "Epoch 45 : Loss = 0.4223391886783217\n",
      "Epoch 46 : Loss = 0.42170877221008846\n",
      "Epoch 47 : Loss = 0.41784794283960947\n",
      "Epoch 48 : Loss = 0.41318522360057797\n",
      "Epoch 49 : Loss = 0.4132768642509879\n",
      "Correctly identified test cases :  1368\n",
      "Accuracy :  68.4\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(1024,16,2)\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "nn.train(inputs, targets, num_epocs)\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Suruchi Implementation \n",
    "    L2 regularizing. Solves the problem of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNNWithRegulization(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, reg_lambda=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the network with input, output sizes, weights and biases\n",
    "        :param input_dim: input dim\n",
    "        :param hidden_size: number of hidden units\n",
    "        :param output_dim: output dim\n",
    "        :param learning_rate: learning rate alpha\n",
    "        :param reg_lambda: regularization rate lambda\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/self.input_size) # Weight matrix for input to hidden\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/self.hidden_size) # Weight matrix for hidden to output\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the ANN\n",
    "        :param X: input\n",
    "        :return: hidden activations, softmax probabilities for the output\n",
    "        \"\"\"\n",
    "        hidden_activations = np.tanh(np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h)\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s)\n",
    "        return hidden_activations, prob_values\n",
    "    \n",
    "    def _regularize_weights(self, delta_W_hy, delta_W_xh, W_hy, W_xh):\n",
    "        \"\"\"\n",
    "        Add regularization terms to the weights\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param Why: weights from hidden to output\n",
    "        :param Wxh: weights from input to hidden\n",
    "        :return: dWhy, dWxh\n",
    "        \"\"\"\n",
    "        delta_W_hy += self.reg_lambda * W_hy\n",
    "        delta_W_xh += self.reg_lambda * W_xh\n",
    "        return delta_W_hy, delta_W_xh\n",
    "\n",
    "    def _update_parameter(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases during gradient descent\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param dbh: bias derivative from input to hidden\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dby: bias derivative from hidden to output\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.W_xh += -self.learning_rate * delta_W_xh\n",
    "        self.bias_h += -self.learning_rate * delta_bias_h\n",
    "        self.W_hy += -self.learning_rate * delta_W_hy\n",
    "        self.bias_y += -self.learning_rate * delta_bias_y\n",
    "\n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        Implementation of the backpropagation algorithm\n",
    "        :param X: input\n",
    "        :param t: target\n",
    "        :param h_a: hidden activation from forward pass\n",
    "        :param probs: softmax probabilities of output from forward pass\n",
    "        :return: dWxh, dWhy, dbh, dby\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)  # backprop into h\n",
    "        delta_h_error = (1 - hidden_activations * hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        delta_bias_h += delta_h_error\n",
    "        \n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "\n",
    "    def _calc_smooth_loss(self, loss, len_samples, regularization=None):\n",
    "        \"\"\"\n",
    "        Calculate the smoothened loss over the set of examples\n",
    "        :param loss: loss calculated for a sample\n",
    "        :param len_examples: total number of samples in training + validation set\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: smooth loss\n",
    "        \"\"\"\n",
    "        if regularization == 'L2':\n",
    "            # Add regulatization term to loss\n",
    "            loss += self.reg_lambda/2 * (np.sum(np.square(self.W_xh)) + np.sum(np.square(self.W_hy)))\n",
    "            return 1./len_samples * loss\n",
    "        else:\n",
    "            return 1./len_samples * loss\n",
    "\n",
    "    def train(self, inputs, targets, sample_data, num_epochs,model_file = \"NNModel.pkl\", regularization=None):\n",
    "        \"\"\"\n",
    "        Trains the network by performing forward pass followed by backpropagation\n",
    "        :param inputs: list of training inputs\n",
    "        :param targets: list of corresponding training targets\n",
    "        :param validation_data: tuple of (X,y) where X and y are inputs and targets\n",
    "        :param num_epochs: number of epochs for training the model\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for k in range(num_epochs):\n",
    "            loss = 0\n",
    "            for i in range(len(inputs)):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(inputs[i])\n",
    "                loss += -np.log(prob_values[targets[i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inputs[i], targets[i], hidden_activations, prob_values)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            for i in range(len(sample_data[0])):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(sample_data[0][i])\n",
    "                loss += -np.log(prob_values[sample_data[1][i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(sample_data[0][i], sample_data[1][i], hidden_activations, prob_values)\n",
    "\n",
    "                if regularization == 'L2':\n",
    "                    delta_W_hy, delta_W_xh = self._regularize_weights(delta_W_hy, delta_W_xh, self.W_hy, self.W_xh)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            if k%1 == 0:\n",
    "                print(\"Epoch \" + str(k) + \" : Loss = \" + str(self._calc_smooth_loss(loss, len(inputs), regularization)))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an input X, emi\n",
    "        :param X: test input\n",
    "        :return: the output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        # return probs\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        Saves the network to a file\n",
    "        :param model_file: name of the file where the network should be saved\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.6069727887784129\n",
      "Epoch 1 : Loss = 0.09155663529892479\n",
      "Epoch 2 : Loss = 0.047123291175041535\n",
      "Epoch 3 : Loss = 0.03277604691604552\n",
      "Epoch 4 : Loss = 0.025917723277330832\n",
      "Epoch 5 : Loss = 0.02198762202178568\n",
      "Epoch 6 : Loss = 0.019488991255837185\n",
      "Epoch 7 : Loss = 0.017791221551063174\n",
      "Epoch 8 : Loss = 0.016583888749264697\n",
      "Epoch 9 : Loss = 0.015696866050650866\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nn = FullyConnectedNNWithRegulization(4,8,4)\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(1000):\n",
    "    num = random.randint(0,3)\n",
    "    inp = np.zeros((4,))\n",
    "    inp[num] = 1\n",
    "    inputs.append(inp)\n",
    "    targets.append(num)\n",
    "\n",
    "nn.train(inputs[:800], targets[:800], (inputs[800:], targets[800:]), 10, regularization='L2')\n",
    "print(nn.predict([1,0,0,0]))\n",
    "print(nn.predict([0,1,0,0]))\n",
    "print(nn.predict([0,0,1,0]))\n",
    "print(nn.predict([0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=FullyConnectedNNWithRegulization(1024,16,2)\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "training_size = int(0.8*len(targets))\n",
    "validation_size = int(0.2*len(targets))\n",
    "nn.train(inputs[:training_size], targets[:training_size],(inputs[training_size:],targets[training_size:]) ,num_epocs)\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
