{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle, random, csv\n",
    "random.seed(1026847926404610461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_file):\n",
    "    \"\"\"\n",
    "    Loads the network from the model_file\n",
    "    :param model_file: file onto which the network is saved\n",
    "    :return: the network\n",
    "    \"\"\"\n",
    "    return pickle.load(open(model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Implementation of fully connected feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedFeedForwardNN(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, activation = 'relu'):\n",
    "        \"\"\"\n",
    "        Initialize the network with input, output sizes, weights and biases\n",
    "        :param input_dim: input dim\n",
    "        :param hidden_size: number of hidden units\n",
    "        :param output_dim: output dim\n",
    "        :param learning_rate: learning rate alpha\n",
    "        :param reg_lambda: regularization rate lambda\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/self.input_size) # Weight matrix for input to hidden\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/self.hidden_size) # Weight matrix for hidden to output\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output bias\n",
    "        self.learning_rate = learning_rate\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = self._sigmoid\n",
    "            self.activation_derivative = self._sigmoid_derivative\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = self._tanh\n",
    "            self.activation_derivative = self._tanh_derivative\n",
    "        else:\n",
    "            self.activation = self._relu\n",
    "            self.activation_derivative = self._relu_derivative\n",
    "    \n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def _sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the ANN\n",
    "        :param X: input\n",
    "        :return: hidden activations, softmax probabilities for the output\n",
    "        \"\"\"\n",
    "        #hidden_activations = np.tanh(np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h)\n",
    "        Z = np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h\n",
    "        hidden_activations = self.activation(Z)\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s)\n",
    "        return hidden_activations, prob_values\n",
    "\n",
    "    def _update_parameter(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases during gradient descent\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param dbh: bias derivative from input to hidden\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dby: bias derivative from hidden to output\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.W_xh += -self.learning_rate * delta_W_xh\n",
    "        self.bias_h += -self.learning_rate * delta_bias_h\n",
    "        self.W_hy += -self.learning_rate * delta_W_hy\n",
    "        self.bias_y += -self.learning_rate * delta_bias_y\n",
    "\n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        Implementation of the backpropagation algorithm\n",
    "        :param X: input\n",
    "        :param t: target\n",
    "        :param h_a: hidden activation from forward pass\n",
    "        :param probs: softmax probabilities of output from forward pass\n",
    "        :return: dWxh, dWhy, dbh, dby\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)  # backprop into h\n",
    "        delta_h_error = self.activation_derivative(hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        #delta_h_error = (1 - hidden_activations * hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        delta_bias_h += delta_h_error\n",
    "        \n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "\n",
    "    def _calc_smooth_loss(self, loss, len_examples):\n",
    "        \"\"\"\n",
    "        Calculate the smoothened loss over the set of examples\n",
    "        :param loss: loss calculated for a sample\n",
    "        :param len_examples: total number of samples in training + validation set\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: smooth loss\n",
    "        \"\"\"\n",
    "        return 1./len_examples * loss\n",
    "\n",
    "    def train(self, inputs, targets, num_epochs,model_file = \"NNModel.pkl\"):\n",
    "        \"\"\"\n",
    "        Trains the network by performing forward pass followed by backpropagation\n",
    "        :param inputs: list of training inputs\n",
    "        :param targets: list of corresponding training targets\n",
    "        :param validation_data: tuple of (X,y) where X and y are inputs and targets\n",
    "        :param num_epochs: number of epochs for training the model\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for k in range(num_epochs):\n",
    "            loss = 0\n",
    "            for i in range(len(inputs)):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(inputs[i])\n",
    "                loss += -np.log(prob_values[targets[i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inputs[i], targets[i], hidden_activations, prob_values)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "\n",
    "            if k%1 == 0:\n",
    "                print(\"Epoch \" + str(k) + \" : Loss = \" + str(self._calc_smooth_loss(loss, len(inputs))))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an input X, emi\n",
    "        :param X: test input\n",
    "        :return: the output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        # return probs\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        Saves the network to a file\n",
    "        :param model_file: name of the file where the network should be saved\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a test cell, to test the working of NN. Need to remove this in the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.5710675894286914\n",
      "Epoch 1 : Loss = 0.11839317034040815\n",
      "Epoch 2 : Loss = 0.03698336488697387\n",
      "Epoch 3 : Loss = 0.018831883017444637\n",
      "Epoch 4 : Loss = 0.011990278935063407\n",
      "Epoch 5 : Loss = 0.00857658363575876\n",
      "Epoch 6 : Loss = 0.00658007219402851\n",
      "Epoch 7 : Loss = 0.005288426410545961\n",
      "Epoch 8 : Loss = 0.00439265776468491\n",
      "Epoch 9 : Loss = 0.0037391395584623907\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nn = FullyConnectedFeedForwardNN(4,8,4, activation ='relu')\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(1000):\n",
    "    num = random.randint(0,3)\n",
    "    inp = np.zeros((4,))\n",
    "    inp[num] = 1\n",
    "    inputs.append(inp)\n",
    "    targets.append(num)\n",
    "\n",
    "nn.train(inputs, targets, 10)\n",
    "print(nn.predict([1,0,0,0]))\n",
    "print(nn.predict([0,1,0,0]))\n",
    "print(nn.predict([0,0,1,0]))\n",
    "print(nn.predict([0,0,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 : Load circles dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"circles500.csv\")\n",
    "index = 0\n",
    "circles_data = []\n",
    "for line in file :\n",
    "    if index == 0:\n",
    "        index +=1\n",
    "        continue\n",
    "    x0,x1,output_class = line.split(',')\n",
    "    current_row = {}\n",
    "    inp = np.asarray([float(x0),float(x1)])\n",
    "    current_row[\"inp\"] = inp\n",
    "    current_row[\"out\"] = int(output_class)\n",
    "    circles_data.append(current_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Run Neural network model on circles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.9500350125569443\n",
      "Epoch 1 : Loss = 0.6938922314870895\n",
      "Epoch 2 : Loss = 0.6508353450240333\n",
      "Epoch 3 : Loss = 0.6207526775829462\n",
      "Epoch 4 : Loss = 0.594560670918524\n",
      "Epoch 5 : Loss = 0.5708922643739173\n",
      "Epoch 6 : Loss = 0.548972685082602\n",
      "Epoch 7 : Loss = 0.527415603211302\n",
      "Epoch 8 : Loss = 0.5076766840945053\n",
      "Epoch 9 : Loss = 0.4873032196452144\n",
      "Epoch 10 : Loss = 0.4655694563538947\n",
      "Epoch 11 : Loss = 0.44293902021655285\n",
      "Epoch 12 : Loss = 0.4183169010147366\n",
      "Epoch 13 : Loss = 0.3907915786167022\n",
      "Epoch 14 : Loss = 0.36242650857240544\n",
      "Epoch 15 : Loss = 0.3324425545171324\n",
      "Epoch 16 : Loss = 0.29633882547430196\n",
      "Epoch 17 : Loss = 0.2635793183376244\n",
      "Epoch 18 : Loss = 0.23744725471050457\n",
      "Epoch 19 : Loss = 0.21856271813984315\n",
      "Epoch 20 : Loss = 0.20289996143972094\n",
      "Epoch 21 : Loss = 0.18974257717033277\n",
      "Epoch 22 : Loss = 0.17824899101194153\n",
      "Epoch 23 : Loss = 0.16834980276145603\n",
      "Epoch 24 : Loss = 0.1596640279615901\n",
      "Epoch 25 : Loss = 0.15197971810734728\n",
      "Epoch 26 : Loss = 0.14509707101023897\n",
      "Epoch 27 : Loss = 0.13897301126522038\n",
      "Epoch 28 : Loss = 0.1335540165467235\n",
      "Epoch 29 : Loss = 0.12851815449539994\n",
      "Epoch 30 : Loss = 0.12396513546459613\n",
      "Epoch 31 : Loss = 0.11981557696555944\n",
      "Epoch 32 : Loss = 0.11601648868844155\n",
      "Epoch 33 : Loss = 0.11248707353384056\n",
      "Epoch 34 : Loss = 0.10910722515765406\n",
      "Epoch 35 : Loss = 0.1057885405709133\n",
      "Epoch 36 : Loss = 0.10273897364611104\n",
      "Epoch 37 : Loss = 0.09993134747383188\n",
      "Epoch 38 : Loss = 0.0970698439731474\n",
      "Epoch 39 : Loss = 0.09425260191177111\n",
      "Epoch 40 : Loss = 0.09168104842047847\n",
      "Epoch 41 : Loss = 0.08930745154445527\n",
      "Epoch 42 : Loss = 0.08700804400032047\n",
      "Epoch 43 : Loss = 0.08487574562564976\n",
      "Epoch 44 : Loss = 0.08288419676628071\n",
      "Epoch 45 : Loss = 0.08102193748153652\n",
      "Epoch 46 : Loss = 0.07922417583801993\n",
      "Epoch 47 : Loss = 0.07755396013008706\n",
      "Epoch 48 : Loss = 0.0759888501915478\n",
      "Epoch 49 : Loss = 0.07450911465407288\n",
      "Accuracy :  100.0\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(2,4,2)\n",
    "inputs=[]\n",
    "targets=[]\n",
    "num_epocs, counter = 50, 0\n",
    "training_size = int(0.9*len(circles_data))\n",
    "for i in range(0,len(circles_data)):\n",
    "    targets.append(circles_data[i]['out'])\n",
    "    inputs.append(circles_data[i]['inp'])\n",
    "nn.train(inputs[:training_size], targets[:training_size], num_epocs, model_file=\"Circles_NN_Model.pkl\")\n",
    "counter=0\n",
    "for j in range(len(circles_data[training_size:])):\n",
    "    s=nn.predict(circles_data[j+training_size]['inp'])\n",
    "    s1=circles_data[j+training_size]['out']\n",
    "    if s == s1:\n",
    "        counter+=1\n",
    "print(\"Accuracy : \",((counter*1.0)/(j+1))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 : Load CIFAR Dataset. \n",
    "Will load cat and deer train(all batches) and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"CIFAR_Dataset\"\n",
    "train_files = [folder+\"/data_batch_\"+str(i) for i in range(1,6) ]\n",
    "test_file = folder+\"/test_batch\"\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "def Convert_rgb_to_grayscale_and_normalize(image_vector):\n",
    "    grayscale_vector = []\n",
    "    individual_spec_length = int(len(image_vector)/3) \n",
    "    for i in range(individual_spec_length):\n",
    "        red_value = image_vector[i]\n",
    "        green_value = image_vector[i + individual_spec_length]\n",
    "        blue_value = image_vector[i+ (2*individual_spec_length)]\n",
    "        # New grayscale image = ( (0.3 * R) + (0.59 * G) + (0.11 * B) ).\n",
    "        grayscale_value = ((0.3*red_value) + (0.59*green_value) + (0.11*blue_value))\n",
    "        grayscale_vector.append(grayscale_value)\n",
    "    return NormalizeData(np.asarray(grayscale_vector))\n",
    "# This function taken from the CIFAR website\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "deer_samples, cat_samples = {}, {}\n",
    "deer_samples['train'], cat_samples['train'] = [], []\n",
    "cat_samples['test'], deer_samples['test'] = [], []\n",
    "for file in train_files :\n",
    "    train_data = unpickle(file)\n",
    "    for i in range(len(train_data[b'labels'])):\n",
    "        if train_data[b'labels'][i] == 3:\n",
    "            cat_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "        if train_data[b'labels'][i] == 4:\n",
    "            deer_samples['train'].append(Convert_rgb_to_grayscale_and_normalize(train_data[b'data'][i].tolist()))\n",
    "\n",
    "test_data = unpickle(test_file)\n",
    "for i in range(0,len(test_data[b'labels'])):\n",
    "    if test_data[b'labels'][i] == 3:\n",
    "        cat_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "    if test_data[b'labels'][i] == 4:\n",
    "        deer_samples['test'].append(Convert_rgb_to_grayscale_and_normalize(test_data[b'data'][i].tolist()))\n",
    "pickle.dump(cat_samples, open(\"cat_samples.pkl\", \"wb\"))\n",
    "pickle.dump(deer_samples, open(\"deer_samples.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CIFAR dataset with neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.6772088976761089\n",
      "Epoch 1 : Loss = 0.6321344642120926\n",
      "Epoch 2 : Loss = 0.6077941608883441\n",
      "Epoch 3 : Loss = 0.5893005392664837\n",
      "Epoch 4 : Loss = 0.581905976441496\n",
      "Epoch 5 : Loss = 0.5766452197037141\n",
      "Epoch 6 : Loss = 0.5733516715776886\n",
      "Epoch 7 : Loss = 0.569251114120924\n",
      "Epoch 8 : Loss = 0.5653865854838434\n",
      "Epoch 9 : Loss = 0.5619617855103695\n",
      "Epoch 10 : Loss = 0.5588451731588293\n",
      "Epoch 11 : Loss = 0.5571108733215544\n",
      "Epoch 12 : Loss = 0.5539469448230354\n",
      "Epoch 13 : Loss = 0.5515048886483177\n",
      "Epoch 14 : Loss = 0.5482896377124371\n",
      "Epoch 15 : Loss = 0.5454031341072747\n",
      "Epoch 16 : Loss = 0.5428533610980713\n",
      "Epoch 17 : Loss = 0.5401453942986268\n",
      "Epoch 18 : Loss = 0.538527932104156\n",
      "Epoch 19 : Loss = 0.536398224686161\n",
      "Epoch 20 : Loss = 0.5342814795974928\n",
      "Epoch 21 : Loss = 0.5320108193116523\n",
      "Epoch 22 : Loss = 0.5311591389787209\n",
      "Epoch 23 : Loss = 0.528707298194548\n",
      "Epoch 24 : Loss = 0.5270380393062113\n",
      "Epoch 25 : Loss = 0.5255146018405144\n",
      "Epoch 26 : Loss = 0.5239004773312612\n",
      "Epoch 27 : Loss = 0.5223851029401312\n",
      "Epoch 28 : Loss = 0.5220795415249492\n",
      "Epoch 29 : Loss = 0.5185344087567311\n",
      "Epoch 30 : Loss = 0.5161749287608054\n",
      "Epoch 31 : Loss = 0.5158191313222382\n",
      "Epoch 32 : Loss = 0.5135188514936513\n",
      "Epoch 33 : Loss = 0.5114232220233131\n",
      "Epoch 34 : Loss = 0.5094993685943064\n",
      "Epoch 35 : Loss = 0.5084320848856483\n",
      "Epoch 36 : Loss = 0.5064225399692786\n",
      "Epoch 37 : Loss = 0.5050926273025522\n",
      "Epoch 38 : Loss = 0.5047586868983758\n",
      "Epoch 39 : Loss = 0.5030240622286933\n",
      "Epoch 40 : Loss = 0.5004005483782911\n",
      "Epoch 41 : Loss = 0.49891207627227363\n",
      "Epoch 42 : Loss = 0.4977641799073693\n",
      "Epoch 43 : Loss = 0.4956978405175811\n",
      "Epoch 44 : Loss = 0.4954709745228782\n",
      "Epoch 45 : Loss = 0.49321932564144927\n",
      "Epoch 46 : Loss = 0.492078837996462\n",
      "Epoch 47 : Loss = 0.4911445423788193\n",
      "Epoch 48 : Loss = 0.4911003191393623\n",
      "Epoch 49 : Loss = 0.4897848230700861\n",
      "Correctly identified test cases :  1300\n",
      "Accuracy :  65.0\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedFeedForwardNN(1024,16,2,activation = 'relu')\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "nn.train(inputs, targets, num_epocs)\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Suruchi Implementation \n",
    "    L2 regularizing. Solves the problem of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNNWithRegulization(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Fully connected feed forward Neural Network. \n",
    "    This implementation implements only one hidden layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01, reg_lambda=0.01, activation = 'relu'):\n",
    "        \"\"\"\n",
    "        Initialize the network with input, output sizes, weights and biases\n",
    "        :param input_dim: input dim\n",
    "        :param hidden_size: number of hidden units\n",
    "        :param output_dim: output dim\n",
    "        :param learning_rate: learning rate alpha\n",
    "        :param reg_lambda: regularization rate lambda\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * np.sqrt(2/self.input_size) # Weight matrix for input to hidden\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * np.sqrt(2/self.hidden_size) # Weight matrix for hidden to output\n",
    "        self.bias_h = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.bias_y = np.zeros((self.output_size, 1)) # output bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = self._sigmoid\n",
    "            self.activation_derivative = self._sigmoid_derivative\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation = self._tanh\n",
    "            self.activation_derivative = self._tanh_derivative\n",
    "        else:\n",
    "            self.activation = self._relu\n",
    "            self.activation_derivative = self._relu_derivative\n",
    "    \n",
    "    def _relu(self,Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    def _tanh(self,Z):\n",
    "        return np.tanh(Z)\n",
    "    def _sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def _relu_derivative(self,Z):\n",
    "        Z[Z<=0] = 0\n",
    "        Z[Z>0] = 1\n",
    "        return Z\n",
    "    def _tanh_derivative(self,Z):\n",
    "        return (1 - Z * Z)\n",
    "    def _sigmoid_derivative(self,Z):\n",
    "        return Z*(1-Z)\n",
    "\n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward pass of the ANN\n",
    "        :param X: input\n",
    "        :return: hidden activations, softmax probabilities for the output\n",
    "        \"\"\"\n",
    "        Z = np.dot(self.W_xh, np.reshape(X,(len(X),1))) + self.bias_h\n",
    "        hidden_activations = self.activation(Z)\n",
    "        y_s = np.exp(np.dot(self.W_hy, hidden_activations) + self.bias_y)\n",
    "        prob_values = y_s/np.sum(y_s)\n",
    "        return hidden_activations, prob_values\n",
    "    \n",
    "    def _regularize_weights(self, delta_W_hy, delta_W_xh, W_hy, W_xh):\n",
    "        \"\"\"\n",
    "        Add regularization terms to the weights\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param Why: weights from hidden to output\n",
    "        :param Wxh: weights from input to hidden\n",
    "        :return: dWhy, dWxh\n",
    "        \"\"\"\n",
    "        delta_W_hy += self.reg_lambda * W_hy\n",
    "        delta_W_xh += self.reg_lambda * W_xh\n",
    "        return delta_W_hy, delta_W_xh\n",
    "\n",
    "    def _update_parameter(self, delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases during gradient descent\n",
    "        :param dWxh: weight derivative from input to hidden\n",
    "        :param dbh: bias derivative from input to hidden\n",
    "        :param dWhy: weight derivative from hidden to output\n",
    "        :param dby: bias derivative from hidden to output\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.W_xh += -self.learning_rate * delta_W_xh\n",
    "        self.bias_h += -self.learning_rate * delta_bias_h\n",
    "        self.W_hy += -self.learning_rate * delta_W_hy\n",
    "        self.bias_y += -self.learning_rate * delta_bias_y\n",
    "\n",
    "    def _back_propagation(self, X, target_class, hidden_activations, prob_values):\n",
    "        \"\"\"\n",
    "        Implementation of the backpropagation algorithm\n",
    "        :param X: input\n",
    "        :param t: target\n",
    "        :param h_a: hidden activation from forward pass\n",
    "        :param probs: softmax probabilities of output from forward pass\n",
    "        :return: dWxh, dWhy, dbh, dby\n",
    "        \"\"\"\n",
    "        delta_W_xh, delta_W_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hy)\n",
    "        delta_bias_h, delta_bias_y = np.zeros_like(self.bias_h), np.zeros_like(self.bias_y)\n",
    "        \n",
    "        delta_y = np.copy(prob_values)\n",
    "        delta_y[target_class] -= 1\n",
    "        delta_W_hy = np.dot(delta_y, hidden_activations.T)\n",
    "        delta_bias_y += delta_y\n",
    "        \n",
    "        delta_h = np.dot(self.W_hy.T, delta_y)  # backprop into h\n",
    "        delta_h_error = self.activation_derivative(hidden_activations) * delta_h # backprop through tanh nonlinearity\n",
    "        delta_bias_h += delta_h_error\n",
    "        \n",
    "        delta_W_xh += np.dot(delta_h_error, np.reshape(X, (len(X), 1)).T)\n",
    "        return delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y\n",
    "\n",
    "    def _calc_smooth_loss(self, loss, len_samples, regularization=None):\n",
    "        \"\"\"\n",
    "        Calculate the smoothened loss over the set of examples\n",
    "        :param loss: loss calculated for a sample\n",
    "        :param len_examples: total number of samples in training + validation set\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: smooth loss\n",
    "        \"\"\"\n",
    "        if regularization == 'L2':\n",
    "            # Add regulatization term to loss\n",
    "            loss += self.reg_lambda/2 * (np.sum(np.square(self.W_xh)) + np.sum(np.square(self.W_hy)))\n",
    "            return 1./len_samples * loss\n",
    "        else:\n",
    "            return 1./len_samples * loss\n",
    "\n",
    "    def train(self, inputs, targets, sample_data, num_epochs,model_file = \"NNModel.pkl\", regularization=None):\n",
    "        \"\"\"\n",
    "        Trains the network by performing forward pass followed by backpropagation\n",
    "        :param inputs: list of training inputs\n",
    "        :param targets: list of corresponding training targets\n",
    "        :param validation_data: tuple of (X,y) where X and y are inputs and targets\n",
    "        :param num_epochs: number of epochs for training the model\n",
    "        :param regularizer_type: type of regularizer like L1, L2, Dropout\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for k in range(num_epochs):\n",
    "            loss = 0\n",
    "            for i in range(len(inputs)):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(inputs[i])\n",
    "                loss += -np.log(prob_values[targets[i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(inputs[i], targets[i], hidden_activations, prob_values)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            for i in range(len(sample_data[0])):\n",
    "                # Forward pass\n",
    "                hidden_activations, prob_values = self._forward_propagation(sample_data[0][i])\n",
    "                loss += -np.log(prob_values[sample_data[1][i], 0])\n",
    "\n",
    "                # Backpropagation\n",
    "                delta_W_xh, delta_W_hy, delta_bias_h, delta_bias_y = self._back_propagation(sample_data[0][i], sample_data[1][i], hidden_activations, prob_values)\n",
    "\n",
    "                if regularization == 'L2':\n",
    "                    delta_W_hy, delta_W_xh = self._regularize_weights(delta_W_hy, delta_W_xh, self.W_hy, self.W_xh)\n",
    "\n",
    "                # Perform the parameter update with gradient descent\n",
    "                self._update_parameter(delta_W_xh, delta_bias_h, delta_W_hy, delta_bias_y)\n",
    "            \n",
    "            if k%1 == 0:\n",
    "                print(\"Epoch \" + str(k) + \" : Loss = \" + str(self._calc_smooth_loss(loss, len(inputs), regularization)))\n",
    "            \n",
    "        self.save(model_file)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Given an input X, emi\n",
    "        :param X: test input\n",
    "        :return: the output class\n",
    "        \"\"\"\n",
    "        hidden_activations, prob_values = self._forward_propagation(X)\n",
    "        # return probs\n",
    "        return np.argmax(prob_values)\n",
    "\n",
    "    def save(self, model_file):\n",
    "        \"\"\"\n",
    "        Saves the network to a file\n",
    "        :param model_file: name of the file where the network should be saved\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pickle.dump(self, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just here to test the above created NN. Should be removed in the final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 1.6111589125018768\n",
      "Epoch 1 : Loss = 1.1155681706546765\n",
      "Epoch 2 : Loss = 0.6914498286388921\n",
      "Epoch 3 : Loss = 0.41345894683015155\n",
      "Epoch 4 : Loss = 0.26681855757713946\n",
      "Epoch 5 : Loss = 0.19003807311009624\n",
      "Epoch 6 : Loss = 0.1465713821792608\n",
      "Epoch 7 : Loss = 0.11983594061085263\n",
      "Epoch 8 : Loss = 0.10223125112392686\n",
      "Epoch 9 : Loss = 0.09000693032523369\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "nn = FullyConnectedNNWithRegulization(4,8,4,activation = 'sigmoid')\n",
    "inputs = []\n",
    "targets = []\n",
    "for i in range(1000):\n",
    "    num = random.randint(0,3)\n",
    "    inp = np.zeros((4,))\n",
    "    inp[num] = 1\n",
    "    inputs.append(inp)\n",
    "    targets.append(num)\n",
    "\n",
    "nn.train(inputs[:800], targets[:800], (inputs[800:], targets[800:]), 10, regularization='L2')\n",
    "print(nn.predict([1,0,0,0]))\n",
    "print(nn.predict([0,1,0,0]))\n",
    "print(nn.predict([0,0,1,0]))\n",
    "print(nn.predict([0,0,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add activation = 'relu' activation = 'sigmoid' and activation = 'tanh' \n",
    "Also try with different hidden layer neurons like 16,24,32\n",
    "May also try different reg_lambda. Maybe 0.01, 0.05, 0.001\n",
    "If have more time and need more thrill try changing epoch to 50,100,150\n",
    "\n",
    "Please write accuracy values here for all configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 0.8408397544910008\n",
      "Epoch 1 : Loss = 0.7771260372899153\n",
      "Epoch 2 : Loss = 0.7496690453700748\n",
      "Epoch 3 : Loss = 0.7421302226733082\n",
      "Epoch 4 : Loss = 0.7369559086058194\n",
      "Epoch 5 : Loss = 0.733992733107598\n",
      "Epoch 6 : Loss = 0.7316099818287766\n",
      "Epoch 7 : Loss = 0.7294390241215375\n",
      "Epoch 8 : Loss = 0.7279853091363935\n",
      "Epoch 9 : Loss = 0.7258437563352804\n",
      "Epoch 10 : Loss = 0.7225448639840865\n",
      "Epoch 11 : Loss = 0.7200122639905867\n",
      "Epoch 12 : Loss = 0.7176054904354143\n",
      "Epoch 13 : Loss = 0.7167209574924347\n",
      "Epoch 14 : Loss = 0.7147666112825061\n",
      "Epoch 15 : Loss = 0.7153740968628881\n",
      "Epoch 16 : Loss = 0.7143383553214933\n",
      "Epoch 17 : Loss = 0.7120940183104457\n",
      "Epoch 18 : Loss = 0.7118555344473881\n",
      "Epoch 19 : Loss = 0.7112121873028133\n",
      "Epoch 20 : Loss = 0.7110898356033494\n",
      "Epoch 21 : Loss = 0.7105156880008586\n",
      "Epoch 22 : Loss = 0.7102801298089656\n",
      "Epoch 23 : Loss = 0.7095130306009473\n",
      "Epoch 24 : Loss = 0.7085770789539537\n",
      "Epoch 25 : Loss = 0.707661436307962\n",
      "Epoch 26 : Loss = 0.7070107007150116\n",
      "Epoch 27 : Loss = 0.7043154071611412\n",
      "Epoch 28 : Loss = 0.7045702743363067\n",
      "Epoch 29 : Loss = 0.7035220648580098\n",
      "Epoch 30 : Loss = 0.7012799381057521\n",
      "Epoch 31 : Loss = 0.7008289152700155\n",
      "Epoch 32 : Loss = 0.7008684714514082\n",
      "Epoch 33 : Loss = 0.700126991686478\n",
      "Epoch 34 : Loss = 0.6998517579054637\n",
      "Epoch 35 : Loss = 0.6989114447248804\n",
      "Epoch 36 : Loss = 0.6976648836922441\n",
      "Epoch 37 : Loss = 0.6986597133296465\n",
      "Epoch 38 : Loss = 0.6977708686425748\n",
      "Epoch 39 : Loss = 0.6948534766099485\n",
      "Epoch 40 : Loss = 0.6966505426050685\n",
      "Epoch 41 : Loss = 0.6938544905006512\n",
      "Epoch 42 : Loss = 0.6941202206705479\n",
      "Epoch 43 : Loss = 0.6954458918628131\n",
      "Epoch 44 : Loss = 0.6962923854605317\n",
      "Epoch 45 : Loss = 0.6948069823742403\n",
      "Epoch 46 : Loss = 0.6950662161152191\n",
      "Epoch 47 : Loss = 0.6954204779480372\n",
      "Epoch 48 : Loss = 0.6953300525090321\n",
      "Epoch 49 : Loss = 0.6931319948602398\n",
      "Correctly identified test cases :  1303\n",
      "Accuracy :  65.14999999999999\n"
     ]
    }
   ],
   "source": [
    "nn=FullyConnectedNNWithRegulization(1024,16,2)\n",
    "inputs, targets=[], []\n",
    "num_epocs, counter = 50, 0\n",
    "for i in range(len(cat_samples['train'])):\n",
    "    inputs.append(cat_samples['train'][i])\n",
    "    targets.append(0)\n",
    "    inputs.append(deer_samples['train'][i])\n",
    "    targets.append(1)\n",
    "training_size = int(0.8*len(targets))\n",
    "validation_size = int(0.2*len(targets))\n",
    "nn.train(inputs[:training_size], targets[:training_size],(inputs[training_size:],targets[training_size:]) ,num_epocs, regularization='L2')\n",
    "for j in range(0,len(deer_samples['test'])):\n",
    "    s=nn.predict(deer_samples['test'][j])\n",
    "    if s == 1 :\n",
    "        counter+=1\n",
    "    s=nn.predict(cat_samples['test'][j])\n",
    "    if s == 0 :\n",
    "        counter+=1\n",
    "print(\"Correctly identified test cases : \",counter)\n",
    "print(\"Accuracy : \",((counter*1.0)/((j+1)*2))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
